{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangu mini model demo\n",
    "---\n",
    "``python version = 3.11``\n",
    "\n",
    "> [08/02/2024]: En este notebook se hace intenta probar el modelo [Pangu Weather mini](https://github.com/rudolfmard/Pangu-Weather-mini/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install timm\n",
    "# !pip install pygrib # En linux\n",
    "# !conda install conda-forge::pygrib # En windows use -y flag\n",
    "# !pip install chardet.\n",
    "# !pip install charset-normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "# Importamos paquete local\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from model import WeatherModel\n",
    "import data_handler as dh\n",
    "import torch\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_path=\"checkpoint.pt\" '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta debe usarse para configurar el uso de GPU\n",
    "\"\"\"\n",
    "if execution_mode == \"single_gpu\":\n",
    "    print(\"Training on single GPU.\")\n",
    "elif execution_mode == \"multi_gpu\":\n",
    "    print(\"Training on multiple GPUs.\")\n",
    "    init_process_group(backend=\"nccl\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid execution mode. Valid values are 'single_gpu' or 'multi_gpu'\")\"\"\"\n",
    "\n",
    "# This environment variable tells PyTorch CUDA allocator not to split memory blocks larger than certain size.\n",
    "# Mitigates GPU memory fragmentation and allows the training of the full original model to fit onto one GPU.\n",
    "\"\"\"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'\"\"\"\n",
    "\n",
    "# Path to checkpoint.pt to continue training from that checkpoint,\n",
    "# if checkpoint.pt does not exist training starts from scratch.\n",
    "\"\"\"checkpoint_path=\"checkpoint.pt\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nW= 832\n",
      "torch.Size([8, 16, 13])\n",
      "mask_windows= torch.Size([8, 1, 8, 2, 13, 1])\n",
      "nW= 832\n",
      "torch.Size([8, 16, 13])\n",
      "mask_windows= torch.Size([8, 1, 8, 2, 13, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 288\n",
      "torch.Size([8, 8, 9])\n",
      "mask_windows= torch.Size([8, 1, 4, 2, 9, 1])\n",
      "nW= 832\n",
      "torch.Size([8, 16, 13])\n",
      "mask_windows= torch.Size([8, 1, 8, 2, 13, 1])\n",
      "nW= 832\n",
      "torch.Size([8, 16, 13])\n",
      "mask_windows= torch.Size([8, 1, 8, 2, 13, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Training parameters:\n",
    "learning_rate (float):   Learning rate of the training, 5e-4 in original Pangu-Weather.\n",
    "max_epochs (int):        Maximum number of epochs for training, 100 in original Pangu-Weather.\n",
    "save_every (int):        Saves a checkpoint every save_every epoch.\n",
    "batch_size (int):        Batch size of the training data, 1 in original Pangu-Weather.\n",
    "\"\"\"\n",
    "learning_rate = 5e-4\n",
    "max_epochs = 10\n",
    "save_every = 2\n",
    "batch_size = 1\n",
    "\n",
    "\"\"\"\n",
    "Model parameters:\n",
    "C (int):                Dimensionality of patch embedding of the tokens. 192 in original Pangu-Weather. Make sure C is divisible by n_heads.\n",
    "depth (list[int]):      List with length of 4, defines the number of transformer blocks in each 4 EarthSpecificLayers. [2,6,6,2] in original Pangu-Weather.\n",
    "n_heads (list[int]):    List with length of 4, defines the number of heads in transformer blocks of each 4 EarthSpecificLayers. [6, 12, 12, 6] in original Pangu-Weather.\n",
    "D (int):                Dimensionality multiplier of hidden layer in transformer MLP. 4 in original Pangu-Weather.\n",
    "\"\"\"\n",
    "C = 192\n",
    "depth = [2, 6, 6, 2]\n",
    "n_heads = [6, 12, 12, 6]\n",
    "D = 4\n",
    "\n",
    "# Create a model object:\n",
    "model = WeatherModel(C, depth, n_heads, D, batch_size, log_GPU_mem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[09/02/2024]\n",
    "`C:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\timm\\layers\\drop.py`\n",
    "\n",
    "> **line 170:** \n",
    "> ```python\n",
    "> return f'drop_prob={self.drop_prob} ====> return f'drop_prob={round(self.drop_prob.item(),3):0.3f}\n",
    "```\n",
    "fix: ``.item()`` method added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherModel(\n",
       "  (input_layer): PatchEmbedding(\n",
       "    (conv_air): Conv3d(5, 192, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       "    (conv_surface): Conv2d(7, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (layer1): EarthSpecificLayer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (linear): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.000)\n",
       "      )\n",
       "      (1): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (linear): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.029)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): EarthSpecificLayer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.057)\n",
       "      )\n",
       "      (1): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.086)\n",
       "      )\n",
       "      (2): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.114)\n",
       "      )\n",
       "      (3): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.143)\n",
       "      )\n",
       "      (4): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.171)\n",
       "      )\n",
       "      (5): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): EarthSpecificLayer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.057)\n",
       "      )\n",
       "      (1): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.086)\n",
       "      )\n",
       "      (2): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.114)\n",
       "      )\n",
       "      (3): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.143)\n",
       "      )\n",
       "      (4): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.171)\n",
       "      )\n",
       "      (5): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): EarthSpecificLayer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (linear): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.000)\n",
       "      )\n",
       "      (1): EarthSpecificBlock(\n",
       "        (attention): EarthAttention3D(\n",
       "          (linear_qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (linear): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.029)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downsample): DownSample(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "  )\n",
       "  (upsample): UpSample(\n",
       "    (linear1): Linear(in_features=384, out_features=768, bias=False)\n",
       "    (linear2): Linear(in_features=192, out_features=192, bias=False)\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_layer): PatchRecovery(\n",
       "    (tconv_air): ConvTranspose3d(384, 5, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       "    (tconv_surface): ConvTranspose2d(384, 4, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5_air = xr.load_dataset(\n",
    "#    r\"C:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\DB\\2010-2023_01_10-6h-64x32_vars_filtered.nc\")\n",
    "# ERA5_air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_train = round(ERA5_air.time.values.size * 0.7)\n",
    "# ERA5_air_train = ERA5_air.isel(time=slice(0, i_train))\n",
    "# ERA5_air_val = ERA5_air.isel(time=slice(i_train, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5_surf = xr.load_dataset(\n",
    "#    r\"C:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\DB\\2010-2023_01_10-6h-64x32_surfvars_filtered.nc\")\n",
    "# ERA5_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_train = round(ERA5_surf.time.values.size * 0.7)\n",
    "# ERA5_surf_train = ERA5_surf.isel(time=slice(0, i_train))\n",
    "# ERA5_surf_val = ERA5_surf.isel(time=slice(i_train, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = ERA5_air_train.chunk({'time': 14})  # .expand_dims('batch')\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.chunk.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_coords = xr.DataArray(\n",
    "#    range(test.chunk.__sizeof__()), dims=('batch',), name='batch')\n",
    "# batch_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.coords['batch'] = batch_coords\n",
    "# test.dims.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr.DataArray(range(len(test.batch)), dims=('batch',), name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hunk_coords = xr.DataArray(\n",
    "#    range(len(ERA5_air_train.time)), dims=('chunk',), name='chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.as_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = list(range(len(ERA5_air_train.batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el objeto xarray a un tensor de PyTorch\n",
    "# ERA5_air_train_tensor = torch.from_numpy(ERA5_air_train.values)\n",
    "#\n",
    "# Guardar el tensor en un archivo .pt\n",
    "# torch.save(ERA5_air_train_tensor,\n",
    "#           r\"C:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\DB\\Pangu_data\\ERA5_air_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga de datos demo\n",
    "---\n",
    "La descarga de los datos se realiza utilizando el API  [Climate Data Store (CDS) Application Program Interface (API)](https://cds.climate.copernicus.eu/api-how-to). Para instalar el API en Windows se siguieron las instrucciones de [Use CDS API on Windows](https://confluence.ecmwf.int/display/CKB/How+to+install+and+use+CDS+API+on+Windows). Es necesario abrir una cuenta en ``cds.climate.copernicus.eu``.\n",
    "\n",
    "Una ves instalada y configurada el CDS API se debe crear un archivo de credenciales en la ruta: ``%USERPROFILE%\\.cdsapirc``, en su entorno windows, ``%USERPROFILE%`` suele estar en la carpeta ``C:\\Users\\Username``. Dicho archivo tiene que contener las siguientes credenciales:\n",
    "\n",
    "```Bash\n",
    "url: https://cds.climate.copernicus.eu/api/v2\n",
    "key: 286528:90262cbb-a170-4868-929d-f652daa1d58e\n",
    "```\n",
    "\n",
    "Posteriormente para definir el dataset junto con los dominios espaciales y temporales se utiliza el script: `..\\models\\PanguWeather_mini_model\\utils\\load_data_from_CDS.py`\n",
    "\n",
    "Al ejecutar este script la petición al servidor queda en una cola de espera. Esta petición se puede monitorear desde la cuenta personal ``cds.climate.copernicus.eu`` en [your requests](https://cds.climate.copernicus.eu/cdsapp#!/yourrequests).\n",
    "\n",
    "Al ingresar a este panel de control se visualizará la petición de descarga y el tiempo transcurrido desde la petición. Para el primer dataset se tardó ``2.5 H``\n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:\\\\Users\\\\gcuervo\\\\OneDrive - Universidad de Las Palmas de Gran Canaria\\\\Documents\\\\Doctorado\\\\DB\\\\Pangu_data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gcuervo\\\\OneDrive - Universidad de Las Palmas de Gran Canaria\\\\Documents\\\\Doctorado\\\\DB\\\\Pangu_data\\\\air_test_validation.grib'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir + \"air_test_validation.grib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gcuervo\\\\OneDrive - Universidad de Las Palmas de Gran Canaria\\\\Documents\\\\Doctorado\\\\DB\\\\Pangu_data\\\\air_test_validation.grib'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(data_dir, \"air_test_validation.grib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo air_test_validation.grib ya existe en el directorio C:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\DB\\Pangu_data\\. No es necesario crearlo.\n"
     ]
    }
   ],
   "source": [
    "file = \"air_test_validation.grib\"\n",
    "# Verificar si el archivo ya existe\n",
    "if not os.path.exists(os.path.join(data_dir, file)):\n",
    "    # Si el archivo no existe, ejecutar la función para crearlo\n",
    "    dh.air_grib_to_tensor(file, data_dir)\n",
    "else:\n",
    "    print(\n",
    "        f\"El archivo {file} ya existe en el directorio {data_dir}. No es necesario crearlo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo surface_test_validation.grib ya existe en el directorio C:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\DB\\Pangu_data\\. No es necesario crearlo.\n"
     ]
    }
   ],
   "source": [
    "file = \"surface_test_validation.grib\"\n",
    "# Verificar si el archivo ya existe\n",
    "if not os.path.exists(os.path.join(data_dir, file)):\n",
    "    # Si el archivo no existe, ejecutar la función para crearlo\n",
    "    dh.air_grib_to_tensor(file, data_dir)\n",
    "else:\n",
    "    print(\n",
    "        f\"El archivo {file} ya existe en el directorio {data_dir}. No es necesario crearlo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_data = torch.load(data_dir + \"air_test_validation.pt\")\n",
    "surface_data = torch.load(data_dir + \"surface_test_validation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 13, 1440, 721, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1, 1440, 721, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(air_data_path, surface_data_path):\n",
    "    # Load the tensors:\n",
    "    #    air_data shape:        (Hour, Z=13, H=1440, W=721, C=5)\n",
    "    #    surface data shape:    (Hour, H=1440, W=721, C=4)\n",
    "    air_data = torch.load(air_data_path)\n",
    "    surface_data = torch.load(surface_data_path)\n",
    "\n",
    "    # Create a dictionary to hold the data statistics:\n",
    "    statistics = {}\n",
    "\n",
    "    # Calculate mean and standard deviation:\n",
    "    statistics[\"AIR_MEAN\"] = air_data.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    statistics[\"AIR_SD\"] = air_data.std(dim=(0, 2, 3), keepdim=True)\n",
    "    statistics[\"SURFACE_MEAN\"] = surface_data.mean(dim=(0, 1, 2), keepdim=True)\n",
    "    statistics[\"SURFACE_SD\"] = surface_data.std(dim=(0, 1, 2), keepdim=True)\n",
    "\n",
    "    # Save the statistics dictionary to a file:\n",
    "    dir_path = os.path.dirname(air_data_path)\n",
    "    torch.save(statistics, dir_path + \"/statistics.pt\")\n",
    "    print(\"Training data statistics saved at statistics.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data statistics saved at statistics.pt\n"
     ]
    }
   ],
   "source": [
    "# Define paths to air and surface training data files:\n",
    "# \"../../weather_data/air_test.pt\"\n",
    "air_data_path = data_dir + \"air_test_validation.pt\"\n",
    "# \"../../weather_data/surface_test.pt\"\n",
    "surface_data_path = data_dir + \"surface_test_validation.pt\"\n",
    "\n",
    "# Call calculate_statistics function from the data_handler file:\n",
    "calculate_statistics(air_data_path, surface_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[13/02/2024]\n",
    "\n",
    "Para correr los data loaders es necesario cambiar la ruta del archivo ``statistics.pt`` en la función ``normalize_data()`` del modulo `.\\PanguWeather_mini_model\\data_handler.py`:\n",
    "\n",
    ">**Linea 143:**\n",
    ">\n",
    "> ```Python\n",
    "> statistics_path = \"statistics.pt\" ===> dir_path = \"C:\\\\Users\\\\gcuervo\\\\OneDrive - Universidad de Las Palmas de Gran Canaria\\\\Documents\\\\Doctorado\\\\DB\\\\Pangu_data\\\\\"\n",
    "> statistics_path = dir_path + \"statistics.pt\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[13/02/2024]\n",
    "\n",
    "Para correr los data loaders y que no haya problema con la normalización de los datos de superficie hacer un ``.squeeze()`` del tensor. Por tanto, se modifica la clase ``WeatherDataset`` del modulo `.\\PanguWeather_mini_model\\data_handler.py`:\n",
    "\n",
    ">**Linea 169:**\n",
    ">\n",
    "> ```Python\n",
    "> surface_data = normalize_data(torch.load(surface_data_path)) ===> surface_data = normalize_data(torch.load(surface_data_path).squeeze()).squeeze()\n",
    "```\n",
    "> fix: ``.squeeze()`` method added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_handler' from 'c:\\\\Users\\\\gcuervo\\\\OneDrive - Universidad de Las Palmas de Gran Canaria\\\\Documents\\\\Doctorado\\\\PhD_repo\\\\models\\\\PanguWeather_mini_model\\\\data_handler.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "# Reload the module\n",
    "importlib.reload(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_mode = \"single_gpu\"\n",
    "\n",
    "# Create dataloader objects for training and validation data:\n",
    "\n",
    "\n",
    "train_dataset = dh.WeatherDataset(lead_time=1,\n",
    "                                  # \"../weather_data/air_test.pt\",\n",
    "                                  air_data_path=data_dir + \"air_test_validation.pt\",\n",
    "                                  # \"../weather_data/surface_test.pt\"\n",
    "                                  surface_data_path=data_dir + \"surface_test_validation.pt\",\n",
    "                                  )\n",
    "\n",
    "\n",
    "train_dataloader = dh.prepare_dataloader(train_dataset, batch_size, execution_mode)\n",
    "\n",
    "\n",
    "\n",
    "# If validation_dataloader is set to None, no validation is performed between epochs.\n",
    "\n",
    "\n",
    "validation_dataset = dh.WeatherDataset(lead_time=1,\n",
    "                                       # ../weather_data/air_test_validation.pt\",\n",
    "                                       air_data_path=data_dir + \"air_test_validation.pt\",\n",
    "                                       # ../weather_data/surface_test_validation.pt\"\n",
    "                                       surface_data_path=data_dir + \"surface_test_validation.pt\",\n",
    "                                       )\n",
    "\n",
    "\n",
    "validation_dataloader = dh.prepare_dataloader(validation_dataset, batch_size, execution_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 64, 32, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00053248"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[0][0].element_size() * b1[0][0].numel() / 1000**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create loss loss function and optimizer objects:\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=3e-6)\n",
    "loss_fn = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [14/02/2024]\n",
    " \n",
    " Para hacer un recorte en la latitud y longitud para tener matrices que no desborden la GPU\n",
    " data_handler.py: 170\n",
    "```Python        \n",
    "# self.x_air = air_data[:-lead_time]\n",
    "self.x_air = air_data[:-lead_time, :, :64, :32, :]\n",
    "# self.x_surface = surface_data[:-lead_time]\n",
    "self.x_surface = surface_data[:-lead_time, :64, :32, :]\n",
    "# self.y_air = air_data[lead_time:]\n",
    "self.y_air = air_data[lead_time:, :, :64, :32, :]\n",
    "# self.y_surface = surface_data[lead_time:]\n",
    "self.y_surface = surface_data[lead_time:, :64, :32, :]\n",
    "```\n",
    "Como se modifico el tamaño de las matrices pues tocó modificar el de las mascaras\n",
    "model.py: 9\n",
    "```Python    \n",
    "# land_mask = torch.ones((batch_size, 1, 1440, 721))\n",
    "land_mask = torch.ones((batch_size, 1, 64, 32))\n",
    "# soil_type = torch.ones((batch_size, 1, 1440, 721))\n",
    "soil_type = torch.ones((batch_size, 1, 64, 32))\n",
    "#topography = torch.ones((batch_size, 1, 1440, 721))\n",
    "topography = torch.ones((batch_size, 1, 64, 32))\n",
    "```\n",
    "Como se modifico el tamaño de las matrices tocó modificar la resolucion de entrada de las capas del modelo\n",
    "model.py: 27\n",
    "```Python\n",
    "# Resolucion UH\n",
    "# kernel conv: 4 x 4 con saltos de 4\n",
    "#              1440 / 4 = 360\n",
    "#              721 / 4 = 181\n",
    "# Four main layers:\n",
    "# self.layer1 = EarthSpecificLayer(depth=depth[0], dim=C, input_resolution=(8, 360, 181),\n",
    "#                                 heads=n_heads[0], drop_path_ratio_list=drop_path_list[:2], D=D)\n",
    "# Resolucion UL\n",
    "# kernel conv: 4 x 4 con saltos de 4\n",
    "#              64 / 4 = 16\n",
    "#              32 / 4 = 8\n",
    "self.layer1 = EarthSpecificLayer(depth=depth[0], dim=C, input_resolution=(8, 16, 8),\n",
    "                                heads=n_heads[0], drop_path_ratio_list=drop_path_list[:2], D=D)\n",
    "# Resolucion UH\n",
    "# EarthBLock: 2\n",
    "#              360 / 2 = 180\n",
    "#              181 / 2 = 90.5\n",
    "#self.middleLayers = nn.Sequential(\n",
    "# self.layer2 = EarthSpecificLayer(depth=depth[1], dim=2*C, input_resolution=(8, 180, 91), heads=n_heads[1], drop_path_ratio_list=drop_path_list[2:], D=D)\n",
    "# self.layer3 = EarthSpecificLayer(depth=depth[2], dim=2*C, input_resolution=(8, 180, 91), heads=n_heads[2], drop_path_ratio_list=drop_path_list[2:], D=D)\n",
    "# Resolucion Ul\n",
    "# EarthBLock: 2\n",
    "#              16 / 2 = 8\n",
    "#              8 / 2 = 4\n",
    "self.layer2 = EarthSpecificLayer(depth=depth[1], dim=2*C, input_resolution=(8, 8, 4), heads=n_heads[1], drop_path_ratio_list=drop_path_list[2:], D=D)\n",
    "self.layer3 = EarthSpecificLayer(depth=depth[2], dim=2*C, input_resolution=(8, 8, 4), heads=n_heads[2], drop_path_ratio_list=drop_path_list[2:], D=D)\n",
    "#)\n",
    "# Resolucion UH\n",
    "# kernel conv: 4 x 4 con saltos de 4\n",
    "#              1440 / 4 = 360\n",
    "#              721 / 4 = 181\n",
    "# self.layer4 = EarthSpecificLayer(depth=depth[3], dim=C, input_resolution=(8, 360, 181),\n",
    "#                                 heads=n_heads[3], drop_path_ratio_list=drop_path_list[:2], D=D)\n",
    "# Resolucion UL\n",
    "# kernel conv: 4 x 4 con saltos de 4\n",
    "#              64 / 4 = 16\n",
    "#              32 / 4 = 8\n",
    "self.layer4 = EarthSpecificLayer(depth=depth[3], dim=C, input_resolution=(8, 16, 8),\n",
    "                                heads=n_heads[3], drop_path_ratio_list=drop_path_list[:2], D=D)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 16, 9, 192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 8, 8, 2, 4, 2, 192]' is invalid for input of size 221184",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a trainer object and train the model:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_dataloader, validation_dataloader, loss_fn,\n\u001b[0;32m      7\u001b[0m                   optimizer, max_epochs, save_every, execution_mode, checkpoint_path)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\PhD_repo\\models\\PanguWeather_mini_model\\trainer.py:182\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCuda available: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_run, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Evaluate the model after every epoch:\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_data \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\PhD_repo\\models\\PanguWeather_mini_model\\trainer.py:165\u001b[0m, in \u001b[0;36mTrainer._run_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Loop over batches of training data:\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_i, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Run batch and store batch loss:\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     batch_losses[batch_i] \u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\PhD_repo\\models\\PanguWeather_mini_model\\trainer.py:140\u001b[0m, in \u001b[0;36mTrainer._run_batch\u001b[1;34m(self, data, targets)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Forward pass:\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m--> 140\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_air\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_surface\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output[\u001b[38;5;241m0\u001b[39m], targets_air) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output[\u001b[38;5;241m1\u001b[39m], targets_surface)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data_air\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\PhD_repo\\models\\PanguWeather_mini_model\\model.py:84\u001b[0m, in \u001b[0;36mWeatherModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     82\u001b[0m skip \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Downsample the spatial resolution:    (B, 8, 360, 181, C) -> (B, 8, 180, 91, 2C)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2, x)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#x = checkpoint(self.middleLayers, x)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m###     Decoder     ###\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\miniconda3\\envs\\art1_pyenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gcuervo\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Documents\\Doctorado\\PhD_repo\\models\\PanguWeather_mini_model\\model.py:207\u001b[0m, in \u001b[0;36mDownSample.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Merge four tokens into one to halve H and W dimensions:\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# (B, 8, 360, 182, C) -> (B, 8, 180, 91, 4C)\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 207\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, Z, H\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, W\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mC)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Normalize and halve the number of channels:\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# (B, 8, 180, 91, 4C) -> (B, 8, 180, 91, 2C)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 8, 8, 2, 4, 2, 192]' is invalid for input of size 221184"
     ]
    }
   ],
   "source": [
    "checkpoint_path = data_dir + \"checkpoint.pt\"\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'\n",
    "\n",
    "# Create a trainer object and train the model:\n",
    "\n",
    "trainer = Trainer(model, train_dataloader, validation_dataloader, loss_fn,\n",
    "                  optimizer, max_epochs, save_every, execution_mode, checkpoint_path)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate RMSE on a batch of data:\n",
    "calculate_RMSE = True\n",
    "if calculate_RMSE:\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "\n",
    "        # Fetch a batch from validation dataloader:\n",
    "        data, targets = next(iter(validation_dataloader))\n",
    "\n",
    "        # Move the data to the same device as the model:\n",
    "        data_air, data_surface = data\n",
    "        data_air = data_air.to(device)\n",
    "        data_surface = data_surface.to(device)\n",
    "        targets_air, targets_surface = targets\n",
    "        targets_air = targets_air.to(device)\n",
    "        targets_surface = targets_surface.to(device)\n",
    "        # Make prediction with the model:\n",
    "        output_air, output_surface = model((data_air, data_surface))\n",
    "        # Calculate RMSE of the predictions on unnormalized data:\n",
    "        rmse_values = dh.RMSE((dh.unnormalize_data(output_air), dh.unnormalize_data(output_surface)),\n",
    "                              (dh.unnormalize_data(targets_air), dh.unnormalize_data(targets_surface)), save=True)\n",
    "if execution_mode == \"multi_gpu\":\n",
    "    destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art1_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
